"""
ç­”æ¡ˆç”ŸæˆæœåŠ¡ï¼ˆä»…ç”Ÿæˆï¼Œä¸è¯„æµ‹ï¼‰
ç”¨äº Web/API è°ƒç”¨ï¼šä¸Šä¼ é¢˜å· â†’ ç”Ÿæˆå¤šç­‰çº§ç­”æ¡ˆ â†’ è¿”å›ç”Ÿæˆæ–‡ä»¶åˆ—è¡¨
è¯„æµ‹é˜¶æ®µç”±å‰ç«¯ç¡®è®¤åå¦è¡Œè§¦å‘æ ‡å‡†æ‰¹é˜… APIã€‚
"""

import argparse
import asyncio
import json
import os
import sys
from pathlib import Path

# Ensure we can import from local directory
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from answer_generator import generate_level_answers
from homework_reviewer_v2 import ensure_instance_context


def parse_args():
    parser = argparse.ArgumentParser(description="Answer Generation Service (generate-only)")
    parser.add_argument("--input", required=True, help="Path to blank exam docx")
    parser.add_argument("--output-root", required=True, help="Root for outputs")
    parser.add_argument("--levels", nargs="+",
                        default=["ä¼˜ç§€çš„å›ç­”", "è‰¯å¥½çš„å›ç­”", "ä¸­ç­‰çš„å›ç­”", "åˆæ ¼çš„å›ç­”", "è¾ƒå·®çš„å›ç­”"],
                        help="Levels to generate")

    # LLM Config args
    parser.add_argument("--llm-api-key", default="", help="LLM API Key")
    parser.add_argument("--llm-api-url", default="", help="LLM API URL")
    parser.add_argument("--llm-model", default="", help="LLM Model")

    return parser.parse_args()


class StreamPrinter:
    """JSON è¡Œåè®®è¾“å‡ºï¼Œä¾›å‰ç«¯ SSE è§£æ"""

    @staticmethod
    def log(message: str):
        print(json.dumps({"type": "log", "message": message}, ensure_ascii=False), flush=True)

    @staticmethod
    def error(message: str):
        print(json.dumps({"type": "error", "message": message}, ensure_ascii=False), flush=True)

    @staticmethod
    def progress(current: int, total: int, message: str = ""):
        print(json.dumps({"type": "progress", "current": current, "total": total, "message": message},
                         ensure_ascii=False), flush=True)


async def main():
    args = parse_args()
    printer = StreamPrinter()

    input_path = Path(args.input)
    output_root = Path(args.output_root)

    if not input_path.exists():
        printer.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        return

    # â”€â”€ æ„å»º contextï¼ˆè®¤è¯ä¿¡æ¯å¯é€‰ï¼‰â”€â”€
    # ç¯å¢ƒå˜é‡ç”± API route çš„ childEnv æ³¨å…¥
    authorization = os.getenv("AUTHORIZATION", "")
    cookie_val = os.getenv("COOKIE", "")
    instance_nid = os.getenv("INSTANCE_NID", "")

    context = {}

    if authorization and cookie_val and instance_nid:
        # æœ‰è®¤è¯ä¿¡æ¯ â†’ è·å–å®ä¾‹è¯¦æƒ…ï¼ˆæ”¯æŒäº‘ç«¯è§£æï¼‰
        printer.log("ğŸ”‘ æ­£åœ¨è·å–å®ä¾‹ä¿¡æ¯...")
        context = ensure_instance_context() or {}
        if not context:
            printer.log("âš ï¸ æ— æ³•è·å–å®ä¾‹ä¿¡æ¯ï¼Œå°†ä½¿ç”¨æœ¬åœ°è§£æ")
    else:
        printer.log("â„¹ï¸ æœªæä¾›æ™ºæ…§æ ‘è®¤è¯ä¿¡æ¯ï¼Œå°†ä½¿ç”¨æœ¬åœ°è§£ææ¨¡å¼")

    # æ³¨å…¥ LLM é…ç½®ï¼ˆä¼˜å…ˆ argsï¼Œfallback ç¯å¢ƒå˜é‡ï¼‰
    llm_key = args.llm_api_key or os.getenv("LLM_API_KEY", "")
    llm_url = args.llm_api_url or os.getenv("LLM_API_URL", "")
    llm_model = args.llm_model or os.getenv("LLM_MODEL", "")
    if llm_key:
        context["llm_api_key"] = llm_key
    if llm_url:
        context["llm_api_url"] = llm_url
    if llm_model:
        context["llm_model"] = llm_model
    # è‡ªå®šä¹‰ Promptï¼ˆå‰ç«¯ä¼ å…¥ï¼Œé€šè¿‡ç¯å¢ƒå˜é‡ä¼ é€’ï¼‰
    custom_prompt = os.getenv("CUSTOM_PROMPT", "").strip()
    if custom_prompt:
        context["custom_prompt"] = custom_prompt
        printer.log("ğŸ“ ä½¿ç”¨ç”¨æˆ·è‡ªå®šä¹‰ Prompt æ¨¡æ¿")

    custom_levels = os.getenv("CUSTOM_LEVELS", "").strip()
    if custom_levels:
        context["custom_levels"] = custom_levels
        printer.log("ğŸ“ ä½¿ç”¨ç”¨æˆ·è‡ªå®šä¹‰ç­‰çº§æè¿°")
    printer.log(f"ğŸ”§ LLM Key: {'å·²é…ç½® (' + llm_key[:6] + '...)' if llm_key else 'âŒ æœªé…ç½®'}")
    printer.log(f"ğŸ”§ LLM URL: {llm_url or '(é»˜è®¤)'}")
    printer.log(f"ğŸ”§ LLM Model: {llm_model or '(é»˜è®¤)'}")

    # â”€â”€ ç”Ÿæˆé˜¶æ®µ â”€â”€
    printer.log(f"ğŸš€ å¼€å§‹ç”Ÿæˆå­¦ç”Ÿç­”æ¡ˆ (å…± {len(args.levels)} ä»½)")

    gen_output_dir = output_root / "generated_answers"

    try:
        generated_files = await generate_level_answers(
            exam_docx_path=input_path,
            output_dir=gen_output_dir,
            levels=args.levels,
            context=context,
            custom_prompt=context.get("custom_prompt", ""),
            custom_levels_json=context.get("custom_levels", ""),
        )
    except Exception as e:
        printer.error(f"ç”Ÿæˆé˜¶æ®µå‘ç”Ÿé”™è¯¯: {e}")
        return

    if not generated_files:
        printer.error("æœªèƒ½ç”Ÿæˆä»»ä½•ç­”æ¡ˆæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ LLM é…ç½®ã€‚")
        return

    printer.log(f"âœ… å…¨éƒ¨ç”Ÿæˆå®Œæˆï¼Œå…± {len(generated_files)} ä»½ç­”æ¡ˆ")

    # â”€â”€ è¾“å‡ºç»“æœ â”€â”€
    saved_files = []
    for p in generated_files:
        saved_files.append({
            "name": p.name,
            "path": str(p),
            "relative": str(p.relative_to(output_root)),
        })

    final = {
        "type": "generate_complete",
        "outputRoot": str(output_root),
        "files": saved_files,
    }
    print(json.dumps(final, ensure_ascii=False), flush=True)


if __name__ == "__main__":
    if sys.platform == "win32":
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())
