# 评分 Prompt 优化与新维度建议

## 1. 现状分析：为何评分趋同？

通过分析现有的 Prompt 结构，发现评分趋同（大多集中在 75-85 分的"安全区"）的主要原因如下：

1.  **缺乏严厉的扣分机制**：目前的 Prompt 多采用"正面描述"（如：是否覆盖...），模型倾向于"只要沾边就给分"。缺乏明确的"一票否决"或"阶梯式扣分"标准。
2.  **评价维度重叠**：例如`teaching_strategy`（教学策略）和 `interaction_experience`（交互体验）在"引导性"上存在重叠。模型可能在两个维度因同一个优点重复给分。
3.  **定性描述过多**：标准多为"是否自然"、"是否合理"，模型难以量化差异。
4.  **"好人"偏见**：LLM 作为评测者时，倾向于给出"中庸偏上"的评价，除非有极明显的错误。

## 2. 核心优化思路

### 策略 A：引入"扣分制" (Penalty-Based Scoring)
将评分逻辑从"做得好给多少分"改为"基础分 100，发现一个问题扣多少分"。
*   **示例**：
    *   基础分：100 分
    *   发现一次直接给出答案：-15 分
    *   发现一次幻觉：-20 分
    *   步骤顺序错误：-10 分/次
*   **优势**：拉大分数差距，对错误敏感。

### 策略 B：强制差异化评价 (Forced Differentiation)
要求模型在 Prompt 中明确指出"相比于满分，缺少的具体 15 分扣在哪里"。
*   **Prompt 增加指令**：
    > "除非完美无缺，否则分数不得超过 95。如果你给出了 85 分，必须在 analysis 中列出扣除那 15 分的具体理由（例如：交互机械扣 5 分，知识点遗漏扣 10 分）。"

### 策略 C：量化"硬指标"
将部分定性指标转为定量指标。
*   **示例**：
    *   不做"是否引导得好"，改为"引导轮次/总轮次"的比例。
    *   "知识点覆盖率"：文档中有 5 个关键点，必须识别出覆盖了几个（如 3/5），直接映射分数为 60 分。

---

## 3. 现有维度的优化建议

| 维度 | 存在问题 | 优化方案 |
| :--- | :--- | :--- |
| **教学目标完成度** (Goal Completion) | 容易因"只要完成了流程"就给高分，忽视质量。 | **引入"完成质量系数"**。<br>不仅看是否完成，要看是"独立完成"还是"在不断提示下完成"。学生依靠提示越多，得分越低。 |
| **教学策略** (Strategy) | 与"交互体验"重叠；模型难以判断"引导"的好坏。 | **聚焦"苏格拉底式提问"**。<br>检测是否使用了"反问"、"追问"。如果连续 3 轮陈述句而无疑问句，大幅扣分。 |
| **工作流一致性** (Workflow) | 对"软性"的流程偏移检测不敏感。 | **阶段性核查**。<br>不仅评估整体，要求模型输出"环节 1：通过，环节 2：偏移"。一旦发现"回退"或"乱序"，严厉扣分（如直接不合格）。 |
| **交互体验** (Experience) | 分数最容易趋同，"自然"的标准太主观。 | **拆解为"拟人化"与"机械感"**。<br>定义禁止词汇表（如"请按照"、"我是智能体"）。一旦出现模板句式，每处扣 5 分。 |
| **幻觉控制** (Hallucination) | 目前只看是否冲突。 | **增加"未受限扩展"检测**。<br>如果智能体回答了文档范围外的问题（即使答对了），也要扣分（因为作为教学 Agent 应聚焦课程）。 |
| **鲁棒性** (Robustness) | 难以触发，正常对话体现不出。 | **模拟/检测"对抗性"场景**。<br>重点检查当学生"捣乱"或"回答简短"时，Agent 是否能维持教学节奏，而不是跟着学生跑。 |

---

## 4. 建议新增的评分维度

目前的维度主要关注"过程"和"合规"，建议增加关注"结果"和"个性"的维度。

### 新维度 1：知识内化度 (Knowledge Internalization Verification)
*   **目的**：评测学生（用户）到底学会了没有，反向评估 Agent 的教学效果。
*   **Prompt 逻辑**：
    > "检查对话结束时，学生是否输出了能够证明其掌握知识的语句？或者 Agent 是否进行了确认测试？如果没有确认环节，扣分。"

### 新维度 2：角色沉浸度 (Role Immersion) - *替代/细化交互体验*
*   **目的**：区分"不仅是像人，更要像**特定的人**"。
*   **Prompt 逻辑**：
    > "Agent 设定是'严谨的植物学教授'还是'活泼的学长'？检查用词是否符合由于各异的人设（Profile）。如果设定是教授却用了网络流行语，或设定是学长却过于官腔，均属于人设崩塌。"

### 新维度 3：纠错有效性 (Correction Efficacy)
*   **目的**：专门针对"学生犯错"场景的评测。
*   **Prompt 逻辑**：
    > "找出对话中学生的所有错误回答。Agent 是如何纠正的？
    > A级：指出错误原因 + 引导思考（+分）
    > B级：直接给出正确答案（不扣分也不加分）
    > C级：忽视错误，继续流程（大幅扣分）
    > D级：错误地赞同了学生的错误回答（灾难性扣分）"

## 5. 总结与下一步行动

建议优先执行以下操作以解决"评分趋同"问题：

1.  **修改 Prompt 头部**：加入"扣分制"声明，要求模型从 100 分开始扣除。
2.  **细化 Evidence**：要求 `evidence` 字段不仅仅是引用原话，必须包含（Line Number）和（扣分理由）。
3.  **调整 Temperature**：评测时 LLM 的 Temperature 建议设为 0 或极低值，保证评分逻辑的冷酷和一致性。
