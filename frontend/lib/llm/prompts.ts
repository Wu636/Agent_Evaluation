/**
 * LLM 评测提示词模板
 */

export interface PromptContext {
  teacherDoc: string;
  dialogueText: string;
}

/**
 * 构建维度评测的提示词
 */
export function buildDimensionPrompt(
  dimensionKey: string,
  context: PromptContext
): string {
  const { teacherDoc, dialogueText } = context;

  const prompts: Record<string, string> = {
    teaching_goal_completion: `
# 评测任务:教学目标与任务完成度评测

## 评测对象
你需要评测一个教学智能体与学生的对话,判断智能体是否成功引导学生完成了教师文档中规定的全部教学目标。

**这是一票否决项!如果核心任务未完成,无论对话多流畅,都不能通过。**

## 教师文档(标准答案)
\`\`\`markdown
${teacherDoc}
\`\`\`

## 实际对话记录
\`\`\`json
${dialogueText}
\`\`\`

## 评测要点(请逐项检查)

### 1. 关键能力点覆盖率(40分)
- 是否覆盖文档中定义的**所有核心知识点和操作步骤**?
- 每个环节的关键参数、标准是否都传达到位?
- 是否遗漏了任何必要的教学内容?

### 2. 任务顺序与流程完整性(25分)
- 是否按照文档规定的顺序引导学生完成任务?
- 每个环节之间的过渡是否自然合理?
- 是否有跳步、省略、或顺序错乱?

### 3. 主动引导与节点推进(20分)
- 在关键节点,智能体是否主动发起引导?
- 还是被动等待学生提问?
- 是否能在学生卡壳时给出**恰当的提示**(不直接给答案)?

### 4. 任务收敛与总结(15分)
- 任务完成后,是否有明确的收敛与总结?
- 是否确认学生已理解所有要点?
- 是否有"下一步"的引导或反思?

## 输出要求(严格JSON格式)

\`\`\`json
{
  "score": 85,
  "level": "良好",
  "analysis": "详细分析:智能体完成了文档中的5个主要环节...",
  "evidence": [
    "环节1'母株选择'中,完整传达了3-5年生、直径1.0-1.5cm等关键参数",
    "环节2'环剥操作'中,准确说明了深度至木质部、宽度1.5倍等标准"
  ],
  "issues": [
    "遗漏了'雨天检查透气孔'这一养护要点",
    "第三环节'基质包裹'中,未明确提及透气孔的具体位置要求"
  ],
  "suggestions": [
    "补充完整的养护注意事项清单",
    "在基质包裹环节增加透气孔位置的详细说明"
  ]
}
\`\`\`

**重要提醒:**
- 如果发现**核心任务未完成**(如5个环节只完成3个),分数应<60分
- 如果只是细节遗漏但主体完整,可给70-80分
- 如果全部完成且质量高,可给85-95分
- 不要给100分,总有改进空间

请严格按JSON格式输出,不要有任何多余的文字!
`,

    teaching_strategy: `
# 评测任务:教学策略与引导质量评测

## 核心理念
**教学智能体 ≠ 百科问答机器人**

好的教学不是直接给答案,而是:
- 引导学生思考
- 循序渐进地建立知识体系
- 允许试错,在错误中学习
- 通过追问促进深度理解

## 教师文档
\`\`\`markdown
${teacherDoc}
\`\`\`

## 实际对话记录
\`\`\`json
${dialogueText}
\`\`\`

## 评测要点

### 1. 引导式教学而非直接给答案(30分)
检查对话中:
- ❌ "答案是XXX" → 直接给答案
- ✅ "你觉得应该选择什么样的枝条?" → 引导提问
- ✅ "想想看,为什么要选1-2年生的枝条?" → 启发思考

### 2. 循序渐进,由浅入深(25分)
- 是否从简单概念开始,逐步深入?
- 是否在学生掌握基础后才引入复杂内容?
- 前后知识点的衔接是否合理?

### 3. 追问与反问促进思考(25分)
- 当学生回答后,是否有追问"为什么"?
- 是否用反问激发学生主动思考?
- 例如:"你觉得这样做的目的是什么?"

### 4. 允许试错与纠错技巧(20分)
- 学生答错时,是否直接给标准答案?还是引导找出错误原因?
- 纠错时是否说明原因而非简单纠正?
- 是否鼓励学生再次尝试?

## 输出JSON格式

\`\`\`json
{
  "score": 75,
  "level": "良好",
  "analysis": "智能体整体采用引导式教学...",
  "evidence": [
    "在第2轮对话中,用'请说明母株和枝条的选择标准'引导而非直接告知"
  ],
  "issues": [
    "第5轮学生回答错误后,直接给出了标准答案,未引导学生思考",
    "缺少'为什么'类的深度追问"
  ],
  "suggestions": [
    "在学生答错时,改用'你觉得哪里可能有问题'这样的引导",
    "增加追问环节,如'为什么要选这个直径范围'"
  ]
}
\`\`\`

请严格按JSON格式输出!
`,

    workflow_consistency: `
# 评测任务:对话流程一致性与工作流遵循度

## 评测目标
检查智能体是否严格按照设计的工作流运行,有无异常的跳步、回退、循环等问题。

## 教师文档(预期工作流)
\`\`\`markdown
${teacherDoc}
\`\`\`

## 实际对话记录
\`\`\`json
${dialogueText}
\`\`\`

## 评测要点

### 1. 环节顺序正确性(35分)
- 是否按文档规定的环节顺序执行?
- 有无跳过某个环节?
- 有无环节顺序颠倒?

### 2. 角色一致性(25分)
- 智能体是否保持了预设角色(如"辅助员芍药")?
- 有无角色混乱或不该出现的角色发言?
- 角色转换是否合理?

### 3. 流程收敛性(25分)
- 每个环节是否有明确的结束标志?
- 是否在完成后才进入下一环节?
- 有无在结束节点后仍继续无关对话?

### 4. 异常状态处理(15分)
- 出现异常输入时,是否能回到主流程?
- 有无死循环、无限追问?
- 是否能从偏离中恢复?

## 输出JSON格式

\`\`\`json
{
  "score": 88,
  "level": "良好",
  "analysis": "工作流执行基本规范,5个环节按序完成...",
  "evidence": [
    "环节1→2→3→4→5的顺序完全符合文档",
    "每个环节结束时都有明确的'达标'/'可进入下一环节'标记"
  ],
  "issues": [
    "第3轮和第4轮之间出现了短暂的话题偏离",
    "环节4中有一次回退到环节3的内容"
  ],
  "suggestions": [
    "增强环节间的过渡控制,避免话题偏离"
  ]
}
\`\`\`
`,

    interaction_experience: `
# 评测任务:语言与交互体验

## 评测重点
这里不追求文学性,而是**教学可用性**。

## 对话记录
\`\`\`json
${dialogueText}
\`\`\`

## 评测要点

### 1. 表达清晰度(30分)
- 指令是否明确,无歧义?
- 专业术语是否解释到位?
- 学生能否准确理解意图?

### 2. 机械感与模板化(25分)
- 是否存在明显的模板痕迹?
- 语言是否过于程式化?
- 是否有重复使用相同句式?

### 3. 上下文理解(25分)
- 能否正确理解学生的指代("这个"、"它")?
- 能否承接上一轮对话的内容?
- 有无答非所问的情况?

### 4. 语气适配性(20分)
- 语气是否符合教学场景?
- 是否过于随意或过于冷漠?
- 鼓励与纠错的语气是否恰当?

## 输出JSON格式

\`\`\`json
{
  "score": 82,
  "level": "良好",
  "analysis": "语言表达整体清晰,符合教学场景...",
  "evidence": [
    "第5轮对纠错时语气委婉:'需调整'而非'错误'"
  ],
  "issues": [
    "多次出现'请按照要求...'的模板化表达",
    "第8轮未能理解学生的'那个'指代"
  ],
  "suggestions": [
    "减少模板化用语,增加表达多样性"
  ]
}
\`\`\`
`,

    hallucination_control: `
# 评测任务:幻觉与不当输出控制

## 教师文档(知识边界)
\`\`\`markdown
${teacherDoc}
\`\`\`

## 对话记录
\`\`\`json
${dialogueText}
\`\`\`

## 评测要点

### 1. 知识准确性(40分)
- 是否引用了不存在的概念/工具?
- 参数、数值是否与文档一致?
- 有无自行编造的"标准"?

### 2. 文档一致性(30分)
- 是否与教师文档冲突?
- 是否给出了文档中没有的操作步骤?
- 有无超出文档范围的扩展?

### 3. 权限边界(20分)
- 是否越权添加了教学目标?
- 是否擅自修改了评估标准?
- 是否保持在"辅助员"角色内?

### 4. 自信度校准(10分)
- 不确定时是否承认不确定?
- 还是错了也很自信?

## 输出JSON格式

\`\`\`json
{
  "score": 65,
  "level": "合格",
  "analysis": "存在一些参数不一致和超纲内容...",
  "evidence": [
    "环节2中生根剂浓度与文档一致(2000mg/L)"
  ],
  "issues": [
    "提到了'多菌灵溶液预防霉菌',但文档中未涉及病虫害防治",
    "基质湿度说了'60%-70%'但文档要求是'70%-80%'"
  ],
  "suggestions": [
    "严格对照文档,不添加文档外内容",
    "所有参数都需与文档完全一致"
  ]
}
\`\`\`
`,

    robustness: `
# 评测任务:鲁棒性与异常处理能力

## 对话记录
\`\`\`json
${dialogueText}
\`\`\`

## 评测要点

### 1. 偏离后的恢复能力(30分)
- 学生不按预期回答时,能否拉回主线?
- 学生答非所问时,如何纠偏?
- 恢复的方式是否自然?

### 2. 重复问题处理(25分)
- 学生重复提问,是否有耐心?
- 会不会给出完全相同的回答?
- 是否换个角度重新解释?

### 3. 循环避免(25分)
- 有无死循环(反复问同一问题)?
- 有无陷入无意义的对话?
- 是否能主动打破僵局?

### 4. 越界请求处理(20分)
- 学生直接要答案,如何处理?
- 学生要求做文档外的事,如何拒绝?
- 拒绝时是否给出合理解释?

## 输出JSON格式

\`\`\`json
{
  "score": 70,
  "level": "合格",
  "analysis": "基本具备异常处理能力,但某些情况下表现不够稳定...",
  "evidence": [
    "在学生回答简短时,能够继续引导而非卡住"
  ],
  "issues": [
    "未检测到学生重复提问的情况,无法评估",
    "学生一次回答过于简略,智能体未追问"
  ],
  "suggestions": [
    "增加对学生异常输入的识别和处理"
  ]
}
\`\`\`
`,
  };

  return prompts[dimensionKey] || "";
}
