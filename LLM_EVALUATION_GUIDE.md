# 基于LLM的智能体评测系统使用指南

## 🎯 系统特点

### 与规则版本的对比

| 特性 | 规则版 | LLM版 |
|------|--------|-------|
| 评测准确性 | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| 语义理解 | ❌ 弱 | ✅ 强 |
| 主观评价 | ❌ 粗糙 | ✅ 精准 |
| 速度 | ⚡ 秒级 | 🐌 分钟级 |
| 成本 | 💰 免费 | 💰💰 按token计费 |
| 稳定性 | ✅ 100%一致 | ⚠️ 有波动 |
| 可解释性 | ✅ 规则明确 | ⚠️ 黑盒 |

### 核心优势

**1. 深度语义理解**
```python
# 规则版无法识别
文档: "用环剥刀环切两圈"
对话: "使用专用工具围绕枝条切割两次"  # ❌ 不匹配

# LLM版能理解同义表达
LLM分析: "两种表述语义相同,都是描述环剥操作" # ✅ 正确识别
```

**2. 教学质量评判**
```python
# 能判断纠错质量
学生: "枝条直径0.5-1cm"
智能体: "需调整为1.0-1.5cm,因为偏细支撑力不足"

LLM评价: "纠错及时准确,且说明了原因,既纠正错误又帮助理解" # ⭐⭐⭐⭐⭐
```

**3. 上下文连贯性**
```python
# 能理解对话承接
轮1: "现在开始环剥操作"
轮2: "它的深度应该到哪里?"  # "它"指环剥

LLM理解: "学生用'它'指代上文的'环剥',上下文连贯" # ✅
```

## 🚀 快速开始

### 1. 环境准备

**安装依赖:**
```bash
pip install requests --break-system-packages
```

> 注意: 现在使用requests库而不是openai,更轻量!

### 2. 配置API(三种方式任选其一)

**方式1: 使用配置向导(推荐✨)**
```bash
python setup_config.py

# 跟随向导输入:
# 1. API Key
# 2. API地址
# 3. 模型名称

# 完成后会自动创建 .env 文件
```

**方式2: 手动创建.env文件**
```bash
# 1. 复制模板
cp .env.template .env

# 2. 编辑.env文件
# LLM_API_KEY=your_api_key_here
# LLM_BASE_URL=http://llm-service.polymas.com/api/openai/v1/chat/completions
# LLM_MODEL=gpt-4o
```

**方式3: 使用环境变量(临时)**
```bash
export LLM_API_KEY="your_key"
export LLM_BASE_URL="http://llm-service.polymas.com/api/openai/v1/chat/completions"
export LLM_MODEL="gpt-4o"
```

### 3. 测试连接

```bash
python setup_config.py
# 选择 "2. 测试API连接"

# 成功会显示:
# ✅ 连接成功!
#    模型回复: 连接成功
#    Token使用: 25
```

### 4. 运行评测

```bash
python llm_evaluation_agent.py 教师文档.docx 对话记录.json

# 输出示例:
# ✓ 找到配置文件: .env
# ✓ 已加载教师文档: 8234 字符
# ✓ 已加载对话记录: 14 轮
# ✓ LLM配置: http://llm-service.polymas.com/... / gpt-4o
#
# ⏳ 正在评测: 教学目标与任务完成度...
#    Token使用: 提示5234 + 生成823 = 总计6057
# ✓ 教学目标与任务完成度: 85.0分 - 良好
# ...
```

### 5. 查看结果

```bash
# 报告文件
dialogue_xxx_llm_evaluation.txt

# 包含:
# - 高管摘要(总分、等级、核心发现)
# - 6个维度的详细分析
# - 每个维度的证据、问题、建议
# - 关键问题汇总
# - 可执行建议(按优先级)
```

## 📊 评测维度详解

### 1. 教学目标与任务完成度 (40% 权重)⚠️ 一票否决

**评测内容:**
- ✓ 是否覆盖文档中所有关键知识点
- ✓ 是否按正确顺序完成各环节
- ✓ 是否主动引导学生推进
- ✓ 是否有明确的总结与收敛

**一票否决规则:**
- 得分 < 60分 → 直接判定为"不合格"
- 即使其他维度再好也无法通过

**示例评分:**
```
85分 - 良好
证据:
  ✓ 完成了5个环节的全部内容
  ✓ 关键参数(直径1.0-1.5cm等)都准确传达
  ✓ 每个环节结束都有明确标记

问题:
  ✗ 遗漏了"雨天检查透气孔"这一养护要点
  ✗ 未明确强调移栽时机的重要性

建议:
  → 补充完整的养护注意事项清单
  → 在总结环节增加关键时机的提醒
```

### 2. 教学策略与引导质量 (20% 权重)

**评测内容:**
- ✓ 是否引导式教学(非直接给答案)
- ✓ 是否循序渐进、由浅入深
- ✓ 是否用追问促进思考
- ✓ 是否允许试错、善用纠错技巧

**好的教学示例:**
```
❌ 直接给答案:
智能体: "枝条直径应该是1.0-1.5cm"

✅ 引导式教学:
智能体: "你觉得应该选择多粗的枝条?想想看,太细或太粗会有什么问题?"
学生: "应该1-2cm?"
智能体: "思路对了!但稍微调整一下,1.0-1.5cm更合适,因为..."
```

### 3. 对话流程一致性与工作流遵循度 (15% 权重)

**评测内容:**
- ✓ 环节顺序是否正确
- ✓ 角色定位是否一致
- ✓ 每个环节是否有明确收敛
- ✓ 有无跳步、回退、循环异常

**异常情况示例:**
```
⚠️ 跳步:
环节1 → 环节3(跳过环节2)

⚠️ 回退:
环节3进行中,突然回到环节1的内容

⚠️ 无收敛:
任务完成后,仍然继续提问,不结束
```

### 4. 语言与交互体验 (10% 权重)

**评测内容:**
- ✓ 表达清晰、无歧义
- ✓ 避免过度模板化
- ✓ 能理解学生的指代和承接
- ✓ 语气符合教学场景

### 5. 幻觉与不当输出控制 (10% 权重)

**常见幻觉类型:**
```
❌ 参数错误:
文档: "湿度70-80%"
对话: "保持湿度60-70%"  # 数值不对

❌ 编造内容:
对话: "使用多菌灵预防霉菌"  # 文档中没提病虫害防治

❌ 越权教学:
对话: "我们还要学习嫁接技术"  # 超出本次实训范围
```

### 6. 鲁棒性与异常处理能力 (5% 权重)

**评测内容:**
- ✓ 学生偏离时能否拉回
- ✓ 重复提问是否耐心
- ✓ 避免死循环
- ✓ 合理拒绝越界请求

## ⚙️ 高级配置

### 修改评测权重

编辑 `llm_evaluation_agent.py`:

```python
DIMENSIONS = {
    "teaching_goal_completion": {
        "weight": 0.50,  # 提高到50%
        "is_veto": True,
        "veto_threshold": 70  # 提高阈值到70
    },
    "teaching_strategy": {
        "weight": 0.15,  # 降低到15%
    },
    # ... 其他维度
}
```

### 使用不同的LLM模型

```python
agent = LLMEvaluationAgent(
    teacher_doc_path=doc,
    dialogue_json_path=json,
    llm_model="gpt-3.5-turbo"  # 更便宜但效果稍弱
    # llm_model="gpt-4"         # 更贵但效果更好
    # llm_model="claude-3-opus" # 使用Claude(需要Anthropic API)
)
```

### 调整温度参数

```python
# 在 _call_llm 方法中修改
def _call_llm(self, prompt: str, temperature: float = 0.1):
    # temperature=0.1  更确定,评分更稳定
    # temperature=0.5  更随机,评价更多样
```

## 💰 成本估算

### Token消耗

**单次评测:**
- 教师文档: ~2000 tokens
- 对话记录: ~3000 tokens  
- 6个维度 × 5000 tokens = 30000 tokens
- LLM响应: ~6000 tokens
- **总计: ~36000 tokens/次**

### 费用估算

**GPT-4 定价(2024):**
- 输入: $0.03/1K tokens
- 输出: $0.06/1K tokens

**单次评测成本:**
```
输入: 30K tokens × $0.03 = $0.90
输出: 6K tokens × $0.06 = $0.36
总计: ~$1.26/次
```

**GPT-3.5-Turbo (便宜10倍):**
- 单次评测: ~$0.13

**批量评测(100个对话):**
- GPT-4: $126
- GPT-3.5-Turbo: $13

### 省钱技巧

1. **混合策略:**
   ```python
   # 初筛用GPT-3.5(便宜)
   if quick_score < 70:
       # 详细评测用GPT-4(准确)
       detailed_eval()
   ```

2. **使用本地模型:**
   ```bash
   # Ollama本地运行
   ollama run llama3:70b
   export OPENAI_BASE_URL="http://localhost:11434/v1"
   ```

3. **缓存结果:**
   - 相同对话不重复评测
   - 保存评测结果供后续分析

## 🔧 故障排查

### 问题1: API调用失败

```python
❌ RuntimeError: LLM API调用失败: Connection timeout

✓ 解决方案:
1. 检查API Key是否正确
   echo $OPENAI_API_KEY
   
2. 检查网络连接
   curl https://api.openai.com/v1/models -H "Authorization: Bearer $OPENAI_API_KEY"
   
3. 使用代理(如果在国内)
   export https_proxy=http://127.0.0.1:7890
```

### 问题2: JSON解析失败

```python
⚠️ JSON解析失败: Expecting value: line 1 column 1

✓ 可能原因:
- LLM没有严格按JSON格式输出
- 温度参数太高,输出不稳定

✓ 解决方案:
1. 降低温度: temperature=0.1
2. 在提示词中强调: "严格按JSON格式输出,不要有任何多余文字"
3. 使用GPT-4(遵循格式能力更强)
```

### 问题3: 评分不稳定

```python
相同对话,两次评测分数差距>10分

✓ 解决方案:
1. 降低temperature到0.1-0.2
2. 多次评测取平均值
3. 在提示词中增加评分标准的细节
```

### 问题4: 成本过高

```python
✓ 解决方案:
1. 使用GPT-3.5-Turbo(便宜10倍)
2. 减少对话长度(只评测关键部分)
3. 使用本地模型(Ollama + Llama3)
4. 批量评测时使用缓存
```

## 📈 效果对比

### 实际案例

**评测对象:** 压条繁殖实训对话

**规则版结果:**
```
总分: 61.7/100 - 合格
问题: 内容准确性0分(参数不匹配)
```

**LLM版结果(预期):**
```
总分: 82.5/100 - 良好

优势:
✓ 教学目标完成度: 85分 - 所有环节完整覆盖
✓ 教学策略: 88分 - 引导式教学,循序渐进

问题:
✗ 幻觉控制: 68分 - 提到了文档外的病虫害防治
✗ 参数有2处与文档不一致

建议:
→ 严格对照文档,不添加额外内容
→ 所有数值参数需要双重核对
```

## 🎯 使用建议

### 什么时候用LLM版?

**✅ 适合场景:**
- 需要精准评估教学质量
- 对话包含复杂的语义表达
- 需要评估主观性指标(如引导质量)
- 对话数量不大(<100个)
- 成本可接受

**❌ 不适合场景:**
- 大批量快速筛选(>1000个)
- 成本敏感
- 需要100%稳定结果
- 只关注客观指标(如参数匹配)

### 推荐工作流

```
1. 规则版快速筛选
   ↓ (得分<70分的)
2. LLM版精细评测
   ↓
3. 人工复核关键案例
   ↓
4. 迭代优化智能体
```

## 🚀 下一步

1. **测试运行:**
   ```bash
   python llm_evaluation_agent.py 示例文档.docx 示例对话.json
   ```

2. **查看报告,理解评测逻辑**

3. **根据需求调整权重和提示词**

4. **批量评测你的对话数据**

5. **基于报告优化智能体**

---

**开始使用LLM驱动的智能评测吧!** 🎓✨
